{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import string\n",
    "\n",
    "df = pd.read_csv(\"datawithsentiments.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "    text = re.sub(r'[^A-Za-z\\s]',r'',text)\n",
    "    text = re.sub(r'\\n',r'',text)\n",
    "    text = re.sub('This comment may be offensive', ' ', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    stop_words = stopwords.words('english')\n",
    "    token_list = []\n",
    "    for word in tokens:\n",
    "        if not word in stop_words:\n",
    "            token_list.append(word)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token, pos=\"v\") for token in tokens]\n",
    "    s = ' '.join([str(elem) for elem in tokens]) \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_comment'] = df['Text'].apply(lambda x: clean(x))\n",
    "l = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bloodstone 28\n",
      "fuck 8\n",
      "kill 5\n",
      "piece 5\n",
      "make 5\n",
      "im 4\n",
      "think 4\n",
      "go 4\n",
      "stab 4\n",
      "take 4\n",
      "dagger 4\n",
      "knife 3\n",
      "could 3\n",
      "deserve 3\n",
      "rain 3\n",
      "crap 3\n",
      "someone 3\n",
      "get 3\n",
      "ordinary 2\n",
      "god 2\n",
      "tell 2\n",
      "like 2\n",
      "seriously 2\n",
      "level 2\n",
      "doesnt 2\n",
      "sword 2\n",
      "wife 2\n",
      "need 2\n",
      "away 2\n",
      "edwina 2\n",
      "favian 2\n",
      "layer 2\n",
      "hurt 2\n",
      "endollon 2\n",
      "blood 2\n",
      "ill 2\n",
      "na 2\n",
      "fate 2\n",
      "gon 2\n",
      "two 2\n",
      "already 1\n",
      "along 1\n",
      "bear 1\n",
      "readers 1\n",
      "wipe 1\n",
      "bet 1\n",
      "wethe 1\n",
      "swear 1\n",
      "read 1\n",
      "wish 1\n",
      "goddess 1\n",
      "rnlike 1\n",
      "past 1\n",
      "somebody 1\n",
      "everyone 1\n",
      "hoop 1\n",
      "bro 1\n",
      "else 1\n",
      "pant 1\n",
      "better 1\n",
      "chapter 1\n",
      "fall 1\n",
      "wine 1\n",
      "didnt 1\n",
      "bloody 1\n",
      "tongue 1\n",
      "wait 1\n",
      "keep 1\n",
      "dont 1\n",
      "isnt 1\n",
      "socket 1\n",
      "chase 1\n",
      "steal 1\n",
      "part 1\n",
      "yall 1\n",
      "gold 1\n",
      "bc 1\n",
      "dickhead 1\n",
      "ass 1\n",
      "ever 1\n",
      "closet 1\n",
      "heal 1\n",
      "hole 1\n",
      "dead 1\n",
      "penis 1\n",
      "gim 1\n",
      "hold 1\n",
      "possible 1\n",
      "slit 1\n",
      "love 1\n",
      "worst 1\n",
      "oh 1\n",
      "behind 1\n",
      "aidon 1\n",
      "er 1\n",
      "well 1\n",
      "weapon 1\n",
      "jealous 1\n",
      "aint 1\n",
      "bone 1\n",
      "dearest 1\n",
      "mess 1\n",
      "word 1\n",
      "message 1\n",
      "arrow 1\n",
      "obviously 1\n",
      "clarify 1\n",
      "metal 1\n",
      "fast 1\n",
      "lot 1\n",
      "favour 1\n",
      "merely 1\n",
      "watch 1\n",
      "sneak 1\n",
      "question 1\n",
      "power 1\n",
      "shouldnt 1\n",
      "amaze 1\n",
      "snap 1\n",
      "daughter 1\n",
      "body 1\n",
      "close 1\n",
      "stick 1\n",
      "impale 1\n",
      "allow 1\n",
      "even 1\n",
      "hand 1\n",
      "shake 1\n",
      "head 1\n",
      "slice 1\n",
      "enough 1\n",
      "right 1\n",
      "give 1\n",
      "deep 1\n",
      "important 1\n",
      "shoot 1\n",
      "unlike 1\n",
      "resemble 1\n",
      "dedicatedly 1\n",
      "one 1\n",
      "slowly 1\n",
      "face 1\n",
      "huge 1\n",
      "dude 1\n",
      "call 1\n",
      "expect 1\n",
      "axe 1\n",
      "thats 1\n",
      "reasonremoves 1\n",
      "sure 1\n",
      "run 1\n",
      "thank 1\n",
      "throne 1\n",
      "knofe 1\n",
      "noblesse 1\n",
      "edollon 1\n",
      "surgeons 1\n",
      "meat 1\n",
      "first 1\n",
      "skin 1\n",
      "though 1\n",
      "bad 1\n",
      "turn 1\n",
      "mark 1\n",
      "overboard 1\n",
      "eye 1\n",
      "youd 1\n",
      "mean 1\n",
      "anyone 1\n",
      "filthy 1\n",
      "lem 1\n",
      "maybe 1\n",
      "outta 1\n",
      "guess 1\n",
      "cut 1\n",
      "stupid 1\n",
      "uhm 1\n",
      "sorry 1\n",
      "ekrite 1\n",
      "ear 1\n",
      "drink 1\n",
      "youre 1\n",
      "expose 1\n",
      "carve 1\n",
      "titan 1\n",
      "let 1\n",
      "fly 1\n",
      "yourscrewed 1\n",
      "see 1\n",
      "bite 1\n",
      "situation 1\n",
      "good 1\n",
      "seem 1\n",
      "years 1\n",
      "bastard 1\n",
      "ugh 1\n",
      "skull 1\n",
      "build 1\n",
      "thousand 1\n",
      "ambrosine 1\n",
      "babe 1\n",
      "whisper 1\n",
      "help 1\n",
      "use 1\n",
      "cry 1\n",
      "earth 1\n",
      "second 1\n",
      "throat 1\n",
      "yes 1\n",
      "stag 1\n",
      "leora 1\n",
      "others 1\n",
      "thanatos 1\n",
      "pull 1\n"
     ]
    }
   ],
   "source": [
    "person = 'bloodstone'\n",
    "cv = CountVectorizer(stop_words=l)\n",
    "df_word_freq = cv.fit_transform(df.clean_comment)\n",
    "top_words = pd.DataFrame(df_word_freq.toarray(), columns=cv.get_feature_names())\n",
    "top_words = top_words[top_words[person] == 1]\n",
    "top_words = pd.DataFrame((top_words.sum()).sort_values(ascending = False)).reset_index()\n",
    "top_words = top_words.rename(columns = {'index': 'word', 0:'frequency'})\n",
    "top_words = top_words[top_words.frequency > 0]\n",
    "#frequency of words in data\n",
    "for i in range(len(top_words)):\n",
    "    print(top_words.loc[i].word, top_words.loc[i].frequency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
